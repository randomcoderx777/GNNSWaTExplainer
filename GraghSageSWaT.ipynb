{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from cuda_test import test_cuda_availability, matrix_multiplication_test\n",
    "# test_cuda_availability()\n",
    "# matrix_multiplication_test(size=1000, runs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dinosaur\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from torch_geometric.data import Data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SWaTGraphSAGE(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers=2):\n",
    "        super(SWaTGraphSAGE, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.convs = nn.ModuleList()\n",
    "        self.convs.append(SAGEConv(in_channels, hidden_channels))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(SAGEConv(hidden_channels, hidden_channels))\n",
    "        self.convs.append(SAGEConv(hidden_channels, out_channels))\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x, edge_index):\n",
    "        for i in range(self.num_layers - 1):\n",
    "            x = self.convs[i](x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = self.dropout(x)\n",
    "            x = self.convs[-1](x, edge_index)\n",
    "        return torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_swat_graph(normal_df, attack_df=None, save_path='graph_data.pt'):\n",
    "    normal_df.columns = normal_df.columns.str.strip()\n",
    "    if attack_df is not None:\n",
    "        attack_df.columns = attack_df.columns.str.strip()\n",
    "    \n",
    "\n",
    "    feature_cols = [col for col in normal_df.columns \n",
    "                   if col not in ['Timestamp', 'Normal/Attack']]\n",
    "    \n",
    "    print(\"\\nnormal_df.columns:\")\n",
    "    print(normal_df.columns.tolist())\n",
    "    print(\"\\nattack_df.columns:\")\n",
    "    print(attack_df.columns.tolist() if attack_df is not None else \"None\")\n",
    "    \n",
    "\n",
    "    node_features = normal_df[feature_cols].values\n",
    "    if attack_df is not None:\n",
    "        attack_features = attack_df[feature_cols].values\n",
    "        node_features = np.vstack([node_features, attack_features])\n",
    "    \n",
    "\n",
    "    edges = []\n",
    "    feature_to_idx = {name: idx for idx, name in enumerate(feature_cols)}\n",
    "    \n",
    "\n",
    "    connections = [\n",
    "    # P1 connections\n",
    "    ('FIT101', 'LIT101'),\n",
    "    ('MV101', 'FIT101'),\n",
    "    ('P101', 'LIT101'),\n",
    "    ('P102', 'FIT101'),\n",
    "\n",
    "    # P2 connections\n",
    "    ('AIT201', 'AIT202'),\n",
    "    ('AIT202', 'AIT203'),\n",
    "    ('FIT201', 'AIT201'),\n",
    "    ('MV201', 'FIT201'),\n",
    "    ('P201', 'FIT201'),\n",
    "    ('P202', 'AIT202'),\n",
    "    ('P203', 'AIT203'),\n",
    "    ('P204', 'FIT201'),\n",
    "    ('P205', 'AIT202'),  \n",
    "    ('P206', 'AIT203'), \n",
    "\n",
    "    # P3 connections\n",
    "    ('DPIT301', 'FIT301'),\n",
    "    ('FIT301', 'LIT301'),\n",
    "    ('MV301', 'FIT301'),\n",
    "    ('MV302', 'LIT301'),\n",
    "    ('MV303', 'FIT301'),\n",
    "    ('MV304', 'LIT301'),\n",
    "    ('P301', 'FIT301'),\n",
    "    ('P302', 'LIT301'),\n",
    "\n",
    "    # P4 connections\n",
    "    ('AIT401', 'AIT402'),\n",
    "    ('FIT401', 'LIT401'),\n",
    "    ('P401', 'FIT401'),\n",
    "    ('P402', 'LIT401'),\n",
    "    ('P403', 'FIT401'),\n",
    "    ('P404', 'LIT401'),\n",
    "    ('UV401', 'FIT401'),\n",
    "\n",
    "    # P5 connections\n",
    "    ('AIT501', 'AIT502'),\n",
    "    ('AIT502', 'AIT503'),\n",
    "    ('AIT503', 'AIT504'),\n",
    "    ('FIT501', 'AIT501'),\n",
    "    ('FIT502', 'AIT502'),\n",
    "    ('FIT503', 'AIT503'),\n",
    "    ('FIT504', 'AIT504'),\n",
    "    ('P501', 'FIT501'),\n",
    "    ('P502', 'FIT502'),\n",
    "    ('PIT501', 'FIT503'),\n",
    "    ('PIT502', 'FIT504'),\n",
    "    ('PIT503', 'FIT503'),\n",
    "\n",
    "    # P6 connections\n",
    "    ('FIT601', 'P601'),\n",
    "    ('P601', 'P602'),\n",
    "    ('P602', 'P603'),\n",
    "\n",
    "    # Cross-process connections\n",
    "    ('LIT101', 'AIT201'),  # P1 -> P2\n",
    "    ('AIT203', 'DPIT301'),  # P2 -> P3\n",
    "    ('LIT301', 'AIT401'),  # P3 -> P4\n",
    "    ('FIT401', 'AIT501'),  # P4 -> P5\n",
    "    ('AIT503', 'FIT601'),  # P5 -> P6\n",
    "    ('LIT301', 'FIT201'),  # P3 -> P2 \n",
    "    ('AIT401', 'DPIT301'),  # P4 -> P3\n",
    "    ('FIT503', 'AIT401'),  # P5 -> P4\n",
    "    ('P205', 'LIT301'),    \n",
    "    ('P206', 'FIT503')     \n",
    "    ]\n",
    "    \n",
    "    print(\"\\nCreate.Edge:\")\n",
    "    for src, dst in connections:\n",
    "        if src in feature_to_idx and dst in feature_to_idx:\n",
    "            i, j = feature_to_idx[src], feature_to_idx[dst]\n",
    "            edges.extend([[i, j], [j, i]])  \n",
    "            print(f\"{src} <-> {dst}\")\n",
    "    \n",
    "    edge_index = torch.tensor(edges, dtype=torch.long).t()\n",
    "    x = torch.tensor(node_features, dtype=torch.float)\n",
    "    \n",
    "    y = torch.zeros(len(node_features))\n",
    "    if attack_df is not None:\n",
    "        y[len(normal_df):] = 1\n",
    "    \n",
    "    torch.save({'x': x, 'edge_index': edge_index}, save_path)\n",
    "    print(f\"x & edge_index save to {save_path}\")\n",
    "    \n",
    "    print(f\"Node {x.size(0)}\")\n",
    "    print(f\"Dim: {x.size(1)}\")\n",
    "    print(f\"Edge: {edge_index.size(1)}\")\n",
    "    \n",
    "    return Data(x=x, edge_index=edge_index, y=y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_graphsage(model, data, epochs=100, lr=0.01):\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = torch.nn.BCELoss()\n",
    "\n",
    "    num_nodes = data.x.size(0)\n",
    "    train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "    test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "    \n",
    "    train_indices = np.random.choice(num_nodes, int(0.8 * num_nodes), replace=False)\n",
    "    train_mask[train_indices] = True\n",
    "    test_mask[~train_mask] = True\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        out = model(data.x, data.edge_index)\n",
    "        loss = criterion(out[train_mask].squeeze(), data.y[train_mask])\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                pred = (out[test_mask] > 0.5).float()\n",
    "                acc = (pred.squeeze() == data.y[test_mask]).float().mean()\n",
    "                print(f'Epoch {epoch+1:03d}, Loss: {loss:.4f}, Test Acc: {acc:.4f}')\n",
    "            model.train()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data.x, data.edge_index)\n",
    "        pred = (out > 0.5).float()\n",
    "        acc = (pred.squeeze() == data.y).float().mean()\n",
    "        \n",
    "        tp = ((pred.squeeze() == 1) & (data.y == 1)).sum()\n",
    "        fp = ((pred.squeeze() == 1) & (data.y == 0)).sum()\n",
    "        tn = ((pred.squeeze() == 0) & (data.y == 0)).sum()\n",
    "        fn = ((pred.squeeze() == 0) & (data.y == 1)).sum()\n",
    "        \n",
    "        precision = tp / (tp + fp)\n",
    "        recall = tp / (tp + fn)\n",
    "        f1 = 2 * precision * recall / (precision + recall)\n",
    "        \n",
    "    return {\n",
    "        'accuracy': acc.item(),\n",
    "        'precision': precision.item(),\n",
    "        'recall': recall.item(),\n",
    "        'f1': f1.item()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normal_df: (495000, 53), attack_df: (449919, 53)\n",
      "\n",
      "normal_df.columns:\n",
      "['Timestamp', 'FIT101', 'LIT101', 'MV101', 'P101', 'P102', 'AIT201', 'AIT202', 'AIT203', 'FIT201', 'MV201', 'P201', 'P202', 'P203', 'P204', 'P205', 'P206', 'DPIT301', 'FIT301', 'LIT301', 'MV301', 'MV302', 'MV303', 'MV304', 'P301', 'P302', 'AIT401', 'AIT402', 'FIT401', 'LIT401', 'P401', 'P402', 'P403', 'P404', 'UV401', 'AIT501', 'AIT502', 'AIT503', 'AIT504', 'FIT501', 'FIT502', 'FIT503', 'FIT504', 'P501', 'P502', 'PIT501', 'PIT502', 'PIT503', 'FIT601', 'P601', 'P602', 'P603', 'Normal/Attack']\n",
      "\n",
      "attack_df.columns:\n",
      "['Timestamp', 'FIT101', 'LIT101', 'MV101', 'P101', 'P102', 'AIT201', 'AIT202', 'AIT203', 'FIT201', 'MV201', 'P201', 'P202', 'P203', 'P204', 'P205', 'P206', 'DPIT301', 'FIT301', 'LIT301', 'MV301', 'MV302', 'MV303', 'MV304', 'P301', 'P302', 'AIT401', 'AIT402', 'FIT401', 'LIT401', 'P401', 'P402', 'P403', 'P404', 'UV401', 'AIT501', 'AIT502', 'AIT503', 'AIT504', 'FIT501', 'FIT502', 'FIT503', 'FIT504', 'P501', 'P502', 'PIT501', 'PIT502', 'PIT503', 'FIT601', 'P601', 'P602', 'P603', 'Normal/Attack']\n",
      "\n",
      "Create.Edge:\n",
      "FIT101 <-> LIT101\n",
      "MV101 <-> FIT101\n",
      "P101 <-> LIT101\n",
      "P102 <-> FIT101\n",
      "AIT201 <-> AIT202\n",
      "AIT202 <-> AIT203\n",
      "FIT201 <-> AIT201\n",
      "MV201 <-> FIT201\n",
      "P201 <-> FIT201\n",
      "P202 <-> AIT202\n",
      "P203 <-> AIT203\n",
      "P204 <-> FIT201\n",
      "P205 <-> AIT202\n",
      "P206 <-> AIT203\n",
      "DPIT301 <-> FIT301\n",
      "FIT301 <-> LIT301\n",
      "MV301 <-> FIT301\n",
      "MV302 <-> LIT301\n",
      "MV303 <-> FIT301\n",
      "MV304 <-> LIT301\n",
      "P301 <-> FIT301\n",
      "P302 <-> LIT301\n",
      "AIT401 <-> AIT402\n",
      "FIT401 <-> LIT401\n",
      "P401 <-> FIT401\n",
      "P402 <-> LIT401\n",
      "P403 <-> FIT401\n",
      "P404 <-> LIT401\n",
      "UV401 <-> FIT401\n",
      "AIT501 <-> AIT502\n",
      "AIT502 <-> AIT503\n",
      "AIT503 <-> AIT504\n",
      "FIT501 <-> AIT501\n",
      "FIT502 <-> AIT502\n",
      "FIT503 <-> AIT503\n",
      "FIT504 <-> AIT504\n",
      "P501 <-> FIT501\n",
      "P502 <-> FIT502\n",
      "PIT501 <-> FIT503\n",
      "PIT502 <-> FIT504\n",
      "PIT503 <-> FIT503\n",
      "FIT601 <-> P601\n",
      "P601 <-> P602\n",
      "P602 <-> P603\n",
      "LIT101 <-> AIT201\n",
      "AIT203 <-> DPIT301\n",
      "LIT301 <-> AIT401\n",
      "FIT401 <-> AIT501\n",
      "AIT503 <-> FIT601\n",
      "LIT301 <-> FIT201\n",
      "AIT401 <-> DPIT301\n",
      "FIT503 <-> AIT401\n",
      "P205 <-> LIT301\n",
      "P206 <-> FIT503\n",
      "x & edge_index save to graph_data.pt\n",
      "Node 944919\n",
      "Dim: 51\n",
      "Edge: 108\n",
      "data.x.size(0): 944919, data.x.size(1): 51\n",
      "data.edge_index.size(1): 108\n",
      "in_channels: 51\n",
      "hidden_channels: 64\n",
      "out_channels: 1\n",
      "SWaTGraphSAGE(\n",
      "  (convs): ModuleList(\n",
      "    (0): SAGEConv(51, 64, aggr=mean)\n",
      "    (1): SAGEConv(64, 1, aggr=mean)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "Epoch 010, Loss: 0.4341, Test Acc: 0.9705\n",
      "Epoch 020, Loss: 0.1193, Test Acc: 0.9903\n",
      "Epoch 030, Loss: 0.0343, Test Acc: 0.9932\n",
      "Epoch 040, Loss: 0.0175, Test Acc: 0.9953\n",
      "Epoch 050, Loss: 0.0115, Test Acc: 0.9965\n",
      "Epoch 060, Loss: 0.0084, Test Acc: 0.9978\n",
      "Epoch 070, Loss: 0.0065, Test Acc: 0.9983\n",
      "Epoch 080, Loss: 0.0053, Test Acc: 0.9988\n",
      "Epoch 090, Loss: 0.0044, Test Acc: 0.9990\n",
      "Epoch 100, Loss: 0.0038, Test Acc: 0.9992\n",
      "\n",
      "Model Performance:\n",
      "accuracy: 0.9996\n",
      "precision: 0.9998\n",
      "recall: 0.9993\n",
      "f1: 0.9995\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    \n",
    "\n",
    "    normal_df = pd.read_csv('processed_data/SWaT_normal.csv')\n",
    "    attack_df = pd.read_csv('processed_data/SWaT_attack.csv')\n",
    "    print(f\"normal_df: {normal_df.shape}, attack_df: {attack_df.shape}\")\n",
    "    \n",
    "    data = create_swat_graph(normal_df, attack_df)\n",
    "    print(f\"data.x.size(0): {data.x.size(0)}, data.x.size(1): {data.x.size(1)}\")\n",
    "    print(f\"data.edge_index.size(1): {data.edge_index.size(1)}\")\n",
    "    \n",
    "    in_channels = data.x.size(1)  \n",
    "    hidden_channels = 64\n",
    "    out_channels = 1\n",
    "    print(f\"in_channels: {in_channels}\")\n",
    "    print(f\"hidden_channels: {hidden_channels}\")\n",
    "    print(f\"out_channels: {out_channels}\")\n",
    "    \n",
    "    model = SWaTGraphSAGE(in_channels, hidden_channels, out_channels)\n",
    "    print(model)\n",
    "    \n",
    "    model = train_graphsage(model, data, epochs=100)\n",
    "    metrics = evaluate_model(model, data)\n",
    "    print(\"\\nModel Performance:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "    \n",
    "    torch.save({'model_state_dict': model.state_dict(),'x': data.x,'edge_index':  data.edge_index}, 'swat_graphsage_model.pt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normal_accuracy: 0.9999\n",
      "attack_accuracy: 0.9993\n",
      " (True Positives): 449582\n",
      " (True Negatives): 494929\n",
      " (False Positives): 71\n",
      " (False Negatives): 337\n",
      " (Precision): 0.9998\n",
      " (Recall): 0.9993\n",
      "F1score: 0.9995\n",
      "anomaly_scores > 0.5: 0.4759\n",
      "anomaly_scores.max(): 1.0000\n",
      "anomaly_scores.min(): 0.0000\n",
      "anomaly_scores.mean(): 0.4759\n",
      "threshold 0.3: 0.9991\n",
      "threshold 0.4: 0.9995\n",
      "threshold 0.5: 0.9996\n",
      "threshold 0.6: 0.9995\n",
      "threshold 0.7: 0.9994\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    predictions = model(data.x, data.edge_index)\n",
    "    pred_labels = (predictions > 0.5).float().numpy().flatten()\n",
    "    true_labels = data.y.numpy()\n",
    "\n",
    "    accuracy = (pred_labels == true_labels).mean()\n",
    "    normal_mask = (true_labels == 0)\n",
    "    attack_mask = (true_labels == 1)\n",
    "    \n",
    "    normal_accuracy = (pred_labels[normal_mask] == true_labels[normal_mask]).mean()\n",
    "    attack_accuracy = (pred_labels[attack_mask] == true_labels[attack_mask]).mean()\n",
    "    \n",
    "    print(f\"normal_accuracy: {normal_accuracy:.4f}\")\n",
    "    print(f\"attack_accuracy: {attack_accuracy:.4f}\")\n",
    "\n",
    "    tp = np.sum((pred_labels == 1) & (true_labels == 1))\n",
    "    tn = np.sum((pred_labels == 0) & (true_labels == 0))\n",
    "    fp = np.sum((pred_labels == 1) & (true_labels == 0))\n",
    "    fn = np.sum((pred_labels == 0) & (true_labels == 1))\n",
    "    \n",
    "    print(f\" (True Positives): {tp}\")\n",
    "    print(f\" (True Negatives): {tn}\")\n",
    "    print(f\" (False Positives): {fp}\")\n",
    "    print(f\" (False Negatives): {fn}\")\n",
    "    \n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    print(f\" (Precision): {precision:.4f}\")\n",
    "    print(f\" (Recall): {recall:.4f}\")\n",
    "    print(f\"F1score: {f1:.4f}\")\n",
    "    \n",
    "    anomaly_scores = predictions.numpy().flatten()\n",
    "    \n",
    "    print(f\"anomaly_scores > 0.5: {(anomaly_scores > 0.5).mean():.4f}\")\n",
    "    print(f\"anomaly_scores.max(): {anomaly_scores.max():.4f}\")\n",
    "    print(f\"anomaly_scores.min(): {anomaly_scores.min():.4f}\")\n",
    "    print(f\"anomaly_scores.mean(): {anomaly_scores.mean():.4f}\")\n",
    "    \n",
    "    thresholds = [0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "    for threshold in thresholds:\n",
    "        pred_at_threshold = (anomaly_scores > threshold).astype(float)\n",
    "        acc_at_threshold = (pred_at_threshold == true_labels).mean()\n",
    "        print(f\"threshold {threshold}: {acc_at_threshold:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_feature_importance(model, data, feature_names):\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    with torch.no_grad():\n",
    "        feature_importance = []\n",
    "        base_output = model(data.x, data.edge_index)\n",
    "        base_pred = (base_output > 0.5).float()\n",
    "        \n",
    "        for i in range(data.x.size(1)):\n",
    "            perturbed_x = data.x.clone()\n",
    "            perturbed_x[:, i] = torch.zeros_like(perturbed_x[:, i])\n",
    "            \n",
    "            new_output = model(perturbed_x, data.edge_index)\n",
    "            new_pred = (new_output > 0.5).float()\n",
    "            \n",
    "            importance = (base_pred != new_pred).float().mean().item()\n",
    "            feature_importance.append(importance)\n",
    "\n",
    "        feature_importance = np.array(feature_importance)\n",
    "        feature_importance = (feature_importance - feature_importance.min()) / (feature_importance.max() - feature_importance.min())\n",
    "\n",
    "        plt.subplot(121)\n",
    "        importance_df = pd.DataFrame({\n",
    "            'Feature': feature_names,\n",
    "            'Importance': feature_importance\n",
    "        }).sort_values('Importance', ascending=True)\n",
    "        \n",
    "        sns.barplot(x='Importance', y='Feature', data=importance_df, \n",
    "                   palette='YlOrRd')\n",
    "        plt.title('Feature Importance')\n",
    "        plt.xlabel('Normalized Importance')\n",
    "        \n",
    "        plt.subplot(122)\n",
    "        G = nx.Graph()\n",
    "        \n",
    "        for i, name in enumerate(feature_names):\n",
    "            G.add_node(i, name=name, importance=feature_importance[i])\n",
    "\n",
    "        edge_index = data.edge_index.numpy()\n",
    "        edges = list(zip(edge_index[0], edge_index[1]))\n",
    "        G.add_edges_from(edges)\n",
    "\n",
    "        pos = nx.spring_layout(G, k=1, iterations=50)\n",
    "\n",
    "        node_sizes = [3000 * G.nodes[node]['importance'] for node in G.nodes()]\n",
    "        node_colors = [G.nodes[node]['importance'] for node in G.nodes()]\n",
    "        \n",
    "        nx.draw_networkx_nodes(G, pos, \n",
    "                             node_size=node_sizes,\n",
    "                             node_color=node_colors,\n",
    "                             cmap=plt.cm.YlOrRd)\n",
    "        nx.draw_networkx_edges(G, pos, alpha=0.2, edge_color='gray')\n",
    "        \n",
    "        labels = {i: f\"{name}\\n{feature_importance[i]:.2f}\" \n",
    "                 for i, name in enumerate(feature_names)}\n",
    "        nx.draw_networkx_labels(G, pos, labels, font_size=8)\n",
    "        \n",
    "        plt.title('Node Relationship Graph\\n(Node size and color indicate importance)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    importance_ranking = [(name, feature_importance[i]) \n",
    "                         for i, name in enumerate(feature_names)]\n",
    "    importance_ranking.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(\"\\n10:\")\n",
    "    for name, importance in importance_ranking[:10]:\n",
    "        print(f\"{name}: {importance:.4f}\")\n",
    "    \n",
    "    process_importance = {}\n",
    "    for name, importance in importance_ranking:\n",
    "        process_num = name[-3:] if name[-3:].isdigit() else name[-2:] if name[-2:].isdigit() else name[-1]\n",
    "        process = f\"P{process_num}\"\n",
    "        if process not in process_importance:\n",
    "            process_importance[process] = []\n",
    "        process_importance[process].append(importance)\n",
    "  \n",
    "    process_avg_importance = {\n",
    "        process: np.mean(importances) \n",
    "        for process, importances in process_importance.items()\n",
    "    }\n",
    "    sorted_processes = sorted(\n",
    "        process_avg_importance.items(), \n",
    "        key=lambda x: x[1], \n",
    "        reverse=True\n",
    "    )\n",
    "    \n",
    "    for process, avg_importance in sorted_processes:\n",
    "        print(f\"{process}: {avg_importance:.4f}\")\n",
    "    \n",
    "    return importance_ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_names(normal_df):\n",
    "    return [col for col in normal_df.columns \n",
    "            if col not in ['Timestamp', 'Normal/Attack']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "normal_df.columns:\n",
      "['Timestamp', 'FIT101', 'LIT101', 'MV101', 'P101', 'P102', 'AIT201', 'AIT202', 'AIT203', 'FIT201', 'MV201', 'P201', 'P202', 'P203', 'P204', 'P205', 'P206', 'DPIT301', 'FIT301', 'LIT301', 'MV301', 'MV302', 'MV303', 'MV304', 'P301', 'P302', 'AIT401', 'AIT402', 'FIT401', 'LIT401', 'P401', 'P402', 'P403', 'P404', 'UV401', 'AIT501', 'AIT502', 'AIT503', 'AIT504', 'FIT501', 'FIT502', 'FIT503', 'FIT504', 'P501', 'P502', 'PIT501', 'PIT502', 'PIT503', 'FIT601', 'P601', 'P602', 'P603', 'Normal/Attack']\n",
      "\n",
      "attack_df.columns:\n",
      "['Timestamp', 'FIT101', 'LIT101', 'MV101', 'P101', 'P102', 'AIT201', 'AIT202', 'AIT203', 'FIT201', 'MV201', 'P201', 'P202', 'P203', 'P204', 'P205', 'P206', 'DPIT301', 'FIT301', 'LIT301', 'MV301', 'MV302', 'MV303', 'MV304', 'P301', 'P302', 'AIT401', 'AIT402', 'FIT401', 'LIT401', 'P401', 'P402', 'P403', 'P404', 'UV401', 'AIT501', 'AIT502', 'AIT503', 'AIT504', 'FIT501', 'FIT502', 'FIT503', 'FIT504', 'P501', 'P502', 'PIT501', 'PIT502', 'PIT503', 'FIT601', 'P601', 'P602', 'P603', 'Normal/Attack']\n",
      "\n",
      "Create.Edge:\n",
      "FIT101 <-> LIT101\n",
      "MV101 <-> FIT101\n",
      "P101 <-> LIT101\n",
      "P102 <-> FIT101\n",
      "AIT201 <-> AIT202\n",
      "AIT202 <-> AIT203\n",
      "FIT201 <-> AIT201\n",
      "MV201 <-> FIT201\n",
      "P201 <-> FIT201\n",
      "P202 <-> AIT202\n",
      "P203 <-> AIT203\n",
      "P204 <-> FIT201\n",
      "P205 <-> AIT202\n",
      "P206 <-> AIT203\n",
      "DPIT301 <-> FIT301\n",
      "FIT301 <-> LIT301\n",
      "MV301 <-> FIT301\n",
      "MV302 <-> LIT301\n",
      "MV303 <-> FIT301\n",
      "MV304 <-> LIT301\n",
      "P301 <-> FIT301\n",
      "P302 <-> LIT301\n",
      "AIT401 <-> AIT402\n",
      "FIT401 <-> LIT401\n",
      "P401 <-> FIT401\n",
      "P402 <-> LIT401\n",
      "P403 <-> FIT401\n",
      "P404 <-> LIT401\n",
      "UV401 <-> FIT401\n",
      "AIT501 <-> AIT502\n",
      "AIT502 <-> AIT503\n",
      "AIT503 <-> AIT504\n",
      "FIT501 <-> AIT501\n",
      "FIT502 <-> AIT502\n",
      "FIT503 <-> AIT503\n",
      "FIT504 <-> AIT504\n",
      "P501 <-> FIT501\n",
      "P502 <-> FIT502\n",
      "PIT501 <-> FIT503\n",
      "PIT502 <-> FIT504\n",
      "PIT503 <-> FIT503\n",
      "FIT601 <-> P601\n",
      "P601 <-> P602\n",
      "P602 <-> P603\n",
      "LIT101 <-> AIT201\n",
      "AIT203 <-> DPIT301\n",
      "LIT301 <-> AIT401\n",
      "FIT401 <-> AIT501\n",
      "AIT503 <-> FIT601\n",
      "LIT301 <-> FIT201\n",
      "AIT401 <-> DPIT301\n",
      "FIT503 <-> AIT401\n",
      "P205 <-> LIT301\n",
      "P206 <-> FIT503\n",
      "x & edge_index save to graph_data.pt\n",
      "Node 944919\n",
      "Dim: 51\n",
      "Edge: 108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dinosaur\\AppData\\Local\\Temp\\ipykernel_20424\\1541783039.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('swat_graphsage_model.pt')\n",
      "C:\\Users\\Dinosaur\\AppData\\Local\\Temp\\ipykernel_20424\\3570383954.py:27: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(x='Importance', y='Feature', data=importance_df,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10:\n",
      "AIT202: 1.0000\n",
      "AIT501: 0.3716\n",
      "DPIT301: 0.1933\n",
      "LIT301: 0.0970\n",
      "FIT201: 0.0899\n",
      "LIT101: 0.0460\n",
      "AIT502: 0.0459\n",
      "PIT502: 0.0390\n",
      "AIT402: 0.0360\n",
      "P501: 0.0359\n",
      "P202: 0.5000\n",
      "P501: 0.1030\n",
      "P301: 0.0600\n",
      "P201: 0.0248\n",
      "P304: 0.0225\n",
      "P502: 0.0217\n",
      "P402: 0.0200\n",
      "P101: 0.0160\n",
      "P504: 0.0155\n",
      "P205: 0.0146\n",
      "P302: 0.0139\n",
      "P503: 0.0114\n",
      "P401: 0.0105\n",
      "P303: 0.0068\n",
      "P203: 0.0064\n",
      "P403: 0.0006\n",
      "P601: 0.0000\n",
      "P102: 0.0000\n",
      "P204: 0.0000\n",
      "P206: 0.0000\n",
      "P404: 0.0000\n",
      "P602: 0.0000\n",
      "P603: 0.0000\n",
      "\n",
      "feature_importance.png\n"
     ]
    }
   ],
   "source": [
    "normal_df = pd.read_csv('processed_data/SWaT_normal.csv')\n",
    "attack_df = pd.read_csv('processed_data/SWaT_attack.csv')\n",
    "\n",
    "feature_names = get_feature_names(normal_df)\n",
    "\n",
    "data = create_swat_graph(normal_df, attack_df)\n",
    "\n",
    "model = SWaTGraphSAGE(\n",
    "    in_channels=data.x.size(1),\n",
    "    hidden_channels=64,\n",
    "    out_channels=1\n",
    ")\n",
    "checkpoint = torch.load('swat_graphsage_model.pt')\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "importance_ranking = analyze_feature_importance(model, data, feature_names)\n",
    "\n",
    "print(\"\\nfeature_importance.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
